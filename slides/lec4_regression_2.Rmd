---
title: "回归分析"
subtitle: "<多变量回归>"
author: "冯凌秉"
institute: "<span style = 'font-size: 70%;'>
  江西财经大学 <br> 
  产业经济研究院</span>"
date: '2020<br><br>
  `r icon::fa("paper-plane")` <feng.lingbing@jxufe.edu.cn>'
output:
  xaringan::moon_reader:
    css: [default, zh-CN.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---
```{r, setup, include=FALSE,echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = ">", 
                      fig.retina = 3, warning = FALSE, message = FALSE)
library(RefManageR)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown", dashed = TRUE)
bib <- ReadBib("bib1.bib")
```

# 忽略变量偏差
在单变量回归中，我们假设只有一个变量X解释Y，并且我们有个很强的假设认为误差项与X变量不相关。

如果有其他重要的变量可以解释Y，而我们没有放在X中，那它们对于Y的影响就进入了误差项。

如果这些被忽略的变量进一步跟X有关系，那么我们第一步的假设就不满足。这就称为忽略变量偏差 (omitted variable bias)。需要满足两个条件

1. X与这个忽略变量相关

2. 这个忽略变量可以影响Y

在这种情况下得到的估计值就是有偏差的
$$\hat{\beta_1} \xrightarrow[]{p} \beta_1 + \rho_{Xu} \frac{\sigma_u}{\sigma_X}$$

这样的偏差是不能通过增大样本量来解决的 $\hat{\beta_1}$是一个有偏和不一致的估计量，即便随着样本量的增加，它的偏差也是系统性和无法纠正的。

---
# OVB 实例
在班级规模对学生成绩的影响的例子中，我们可能忽略了一个变量：学校区域内英语学习者的比例。因为如果一个学生仍然在学习英语，那么相比母语是英语的其他学生来说，它的课程成绩可能更低。同时，英语学习者比例高的区域，其学校班级的规模也可能较大。因此，这个变量可能符合上面的两个条件。
$$TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times PctEL, \rho_{STR,PctEL}\ne 0$$
```{r}
# load the AER package
library(AER)
data(CASchools)   
CASchools$STR <- CASchools$students/CASchools$teachers       
CASchools$score <- (CASchools$read + CASchools$math)/2
cor(CASchools$STR, CASchools$score)
cor(CASchools$STR, CASchools$english)
```

---
# OVB 与多变量回归

$\widehat{\rho}_{STR, Testscore} = -0.2264$，因此OVB可能导致 $\hat\beta_1$ 被低估。
```{r highlight.output=3}
# estimate both regression models
mod <- lm(score ~ STR, data = CASchools) 
mult.mod <- lm(score ~ STR + english, data = CASchools)
coef(summary(mod))
coef(summary(mult.mod))
```

与理论一致的，多变量回归告诉我们，在单变量回归中发现的班级规模对于成绩的单位负影响程度被高估了 (2.28降低到1.1)。

---
# 多变量回归
相比于单变量回归中Y被单一变量解释，多变量回归表示Y可以被多个X的线性组合所表示: 
$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \dots + \beta_k X_{ki} + u_i \ \ , \ \ i=1,\dots,n.$$
参数的估计法依然可以采用OLS。
```{r}
summary(mult.mod)$coef
```

$$\widehat{TestScore} = 686.03 - 1.10 \times STR - 0.65 \times PctEL$$

---
# 3D散点图与回归平面

<iframe src="3dreg.html" width="120%" height="90%" frameBorder="0"></iframe>

---
# 多元回归的拟合优度

多元回归的拟合优度同简单回归一样，我们多采用SER，R方和调整的R方。其中最后一个是多元回归特有的。简单回归不需要调整R方。
$$SER = s_{\hat u} = \sqrt{s_{\hat u}^2}, s_{\hat u}^2 = \frac{1}{n-k-1} \, SSR$$
k是变量X的个数。调整R方
$$\bar{R}^2 = 1-\frac{n-1}{n-k-1} \, \frac{SSR}{TSS}.$$
对R方进行调整的主要原因是，非调整的R方会随着X变量个数的增加而无谓的增加。调整R方可以适度的惩罚变量的个数，得到更为稳健的拟合优度。从调整R方来看，加了一个变量的线性模型比之前的单变量回归解释力度更强。

```{r}
summary(mult.mod)[c("r.squared", "adj.r.squared")]
```

---
# 多元回归假设
简单回归中，我们的三个假设为独立同分布假设，条件均值为0假设，以及没有异常值假设。在多元回归中，我们还假设不存在严重的多重共线性。总结来说：
1. $(X_{1i}, X_{2i}, \dots, X_{ki}, Y_i) \ , \ i=1,\dots,n$ 是独立同分布的样本。
2. $E(u_i\vert X_{1i}, X_{2i}, \dots, X_{ki}) = 0.$。
3. 没有显著的异常值，也就是X和Y具有有限的四阶矩。
4. 没有严重的多重共线性。

多重共线性的发生场景是：
- 某两个变量的相关性非常强，
- 或者某个变量是另外几个变量的线性组合，后者的情形称为完全多重共线性 (perfect multicollinearity)。

### 多重共线性的主要后果是：OLS估计的结果不再可信！

---
# 多重共线性实例（1)
```{r}
CASchools$FracEL <- CASchools$english / 100
mult.mod <- lm(score ~ STR + english + FracEL, data = CASchools) 
summary(mult.mod)
```

---
# 多重共线性实例（2)
```{r}
CASchools$NS <- ifelse(CASchools$STR < 12, 0, 1)
mult.mod <- lm(score ~ computer + english + NS, data = CASchools)
summary(mult.mod)
```



---
# 哑变量陷阱
可以根据学区所在的区域设定如下哑变量:
$$
\begin{align*}
  North_i =& 
  \begin{cases}
    1 \ \ \text{if located in the north} \\
    0 \ \ \text{otherwise}
  \end{cases} \\
    West_i =& 
  \begin{cases}
    1 \ \ \text{if located in the west} \\
    0 \ \ \text{otherwise}
  \end{cases} \\
    South_i =& 
  \begin{cases}
    1 \ \ \text{if located in the south} \\
    0 \ \ \text{otherwise}
  \end{cases} \\
    East_i =& 
  \begin{cases}
    1 \ \ \text{if located in the east} \\
    0 \ \ \text{otherwise}.
  \end{cases}
\end{align*}
$$
如果将四个哑变量放在一个回归模型中作为四个新增的X变量，
$$
TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times english + \beta_3 \times North_i + \beta_4 \times West_i + \beta_5 \times South_i + \beta_6 \times East_i + u_i \tag{6.8}
$$
由于$$North_i + West_i + South_i + East_i = 1.$$
这四个变量将引起完全多重共线性问题。

---
# 哑变量陷阱 （2）

.pull-left[
**如果回归方程中有截距项**，那么
$$
\begin{align}
  intercept = \, & \lambda_1 \cdot (North + West + South + East) \\
  \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} = \, & \lambda_1 \cdot \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} \\   \Leftrightarrow \, & \lambda_1 = 1
\end{align}
$$
下面模拟一个direction变量，并用R建模。可以发现，此处R自动识别出有哑变量陷阱，在实际使用X时，只使用了三个变量，而不是全部的四个方向哑变量。

```{r plot_1, results='hide'}
CASchools$direction <- sample(c("West", "North", "South", "East"), 
                              420, 
                              replace = T)
mult.mod <- lm(score ~ STR + english + direction, data = CASchools)

summary(mult.mod)
```
]

.pull-right[

```{r highlight.output=14:16, ref.label = 'plot_1', echo=F}
```
]

---
# 不完全多重共线性
在处理多重共线性时，存在一个*度*的问题。因为X变量之间存在一定程度的相关关系使我们使用多变量回归的主要原因。

如果X变量之间完全不相关，那么我们完全可以对每一个X变量，建立跟Y单独的单变量简单回归。

假设我们有回归模型: $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i$$
则
$$\sigma^2_{\hat\beta_1} = \frac{1}{n} \left( \frac{1}{1-\rho^2_{X_1,X_2}} \right) \frac{\sigma^2_u}{\sigma^2_{X_1}}$$
- 如果 $\rho_{X_1,X_2}=0$，那么在回归方程中加入 $X_2$ 对 $\hat{\beta_1}$ 估计的有效性没有影响。
- 如果相关系数越大， $\hat{\beta_1}$ 估计的有效性越弱。
- 增加样本量 $n$ 有助于提高估计的有效性。

---
# 偏差-方差平衡
### 在多重回归中，增加一个额外的解释变量，将使得已有变量估计的有效性降低，也就是估计标准误增加。这叫做 variance inflation。

### 但是确实有用的解释变量不加入模型，又可能导致可能的忽略变量偏误 (OVB) 问题。

### 因此，是否加入某些解释变量，这里面有一个平衡的问题，在统计学习中这通常叫做*偏差-方差平衡 (bias-variance trade-off)*。

### 因此不加入有用的X会导致 (bias) 偏差，而加入则增加了 (variance) 方差

---
# 偏差-方差平衡：模拟实例
```{r}
library(MASS)
library(mvtnorm)
n <- 50
coefs1 <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))
coefs2 <- coefs1
set.seed(1)
for (i in 1:10000) {
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs1[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 8.5), c(8.5, 10)))
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs2[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
}
diag(var(coefs1))
diag(var(coefs2))
```

---
# 多元回归系数的抽样分布
在OLS假设得到满足的条件下，多元回归的系数是服从联合正态分布的。
下面用一个模拟来验证。模拟步骤为：
1. 从如下回归方程中生成100000组样本量为50的数据样本。
$$Y_i = 5 + 2.5 \cdot X_{1i} + 3 \cdot X_{2i} + u_i, u_i \sim \mathcal{N}(0,5)$$
其中X变量的关系为
$$X_i = (X_{1i}, X_{2i}) \sim \mathcal{N} \left[\begin{pmatrix} 0 \\ 0  \end{pmatrix}, \begin{pmatrix} 10 & 2.5 \\ 2.5 & 10 \end{pmatrix} \right]$$
2. 对10万组数据，每一组都拟合如下模型并估计参数 $\hat{\beta_1},\hat{\beta_2}$:

$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i$$
3. 使用`kde2d()` 函数计算 $(\hat{\beta_1},\hat{\beta_2})$联合分布的密度估计。

---
# 联合分布模拟
.pull-left[
```{r plot_2, fig.show=F}
library(MASS)
library(mvtnorm)
n <- 50
coefs <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))
set.seed(1)
for (i in 1:10000) {
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs[i,] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
}
kde <- kde2d(coefs[, 1], coefs[, 2])
persp(kde, 
      theta = 310, 
      phi = 30, 
      xlab = "beta_1", 
      ylab = "beta_2", 
      zlab = "Est. Density")
```
]

.pull-right[

```{r ref.label = 'plot_2', echo=F, fig.height=7}
```
]

---
# 联合分布3D图
<iframe src="3dreg_joint_dist.html" width="100%" height="90%" frameBorder="0"></iframe>

---
# 回归系数估计相关
两个X变量的系数估计是相关的:

```{r}
cor(coefs[, 1], coefs[, 2])
cor(X[, 1], X[, 2])
```

$\hat{\beta_1}$ 和 $\hat{\beta_2}$ 的相关性源于两个自变量 $X_1$ 和 $X_2$ 的相关性。

$X_1$ 和 $X_2$ 的正相关性将导致 $\hat{\beta_1}$ 和 $\hat{\beta_2}$ 的相关性为负。[见教材6.2节附录]

---
# 多元回归系数估计假设检验
```{r}
model <- lm(score ~ STR + english, data = CASchools)
coeftest(model, vcov. = vcovHC, type = "HC1")
```

---
# 系数估计置信区间
```{r}
model <- lm(score ~ STR + english, data = CASchools)
confint(model)
```
以上非稳健的置信区间估计，以下是
```{r}
rob_se <- diag(vcovHC(model, type = "HC1"))^0.5
rbind("lower" = coef(model) - qnorm(0.975) * rob_se,
      "upper" = coef(model) + qnorm(0.975) * rob_se)
```

---
# 增加X变量的可能后果

估计以下模型，发现原先显著的size的系数估计，此时不再显著。

$$TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times english + \beta_3 \times expenditure + u$$
```{r}
CASchools$expenditure <- CASchools$expenditure/1000
model <- lm(score ~ STR + english + expenditure, data = CASchools)
coeftest(model, vcov. = vcovHC, type = "HC1")
```

为什么？
```{r}
cor(CASchools$STR, CASchools$expenditure)
```

**不完全多重共线性的后果**

当我们向多元回归模型中添加解释变量时，可能会改变已有变量的显著性（从显著到不显著）。

---
# 联合检验与F统计量
上述估计模型为:
$$
\widehat{TestScore} = \underset{(15.21)}{649.58} -\underset{(0.48)}{0.29} \times STR - \underset{(0.04)}{0.66} \times english + \underset{(1.41)}{3.87} \times expenditure.
$$
在多元回归中，对单个系数估计我们一般采用t检验。但如果我们想要检验size和expenditure是否同时为0，该使用什么检验呢？[教材7.2节]

使用如下的F统计量
$$F = \frac{(SSR_{\text{restricted}} - SSR_{\text{unrestricted}})/q}{SSR_{\text{unrestricted}} / (n-k-1)}$$
$SSR_{\text{restricted}}$ 是限制模型的离差平方和, $SSR_{\text{rerestricted}}$ 是非限制模型的离差平方和, $q$是限制参数的个数, $k$ 是全模型（非限制）参数个数。

F统计量的原理？

---
# 联合检验的R操作
```{r}
model <- lm(score ~ STR + english + expenditure, data = CASchools)
linearHypothesis(model, c("STR=0", "expenditure=0"))
```

---
# 联合检验的R操作：稳健版
```{r}
linearHypothesis(model, c("STR=0", "expenditure=0"), white.adjust = "hc1")
```

---
# 全局F检验
对于`lm`的结果，`summary`函数的输出中也包含了一个F检验
```{r}
summary(model)$fstatistic
```
这个检验称为全局F检验，它的原假设和备择假设为:
$$
H_0: \beta_1=0, \ \beta_2 =0, \ \beta_3 =0 \quad \text{vs.} \quad H_1: \beta_j \neq 0 \ \text{for at least one} \ j=1,2,3.
$$
---
# 全局F检验与限制模型F检验等同
```{r}
linearHypothesis(model, c("STR=0", "english=0", "expenditure=0"))
```

---
# 参数的置信集合
.pull-left[
单变量回归中可以给出每个参数估计的置信区间

多元回归中可以给出两个参数估计的置信集合 (confidence set)

表现为一个椭圆 (Ellipse)
```{r plot_conf_set, fig.show='hide'}
confidenceEllipse(model, 
                  fill = T,
                  lwd = 0,
                  which.coef = c("STR", "expenditure"),
                  main = "95% Confidence Set")    
```
]

.pull-right[
```{r ref.label = 'plot_conf_set', echo=F, fig.height=7}
```
]

---
# 置信集合稳健版
.pull-left[
稳健版的标准误估计一般要大于OLS标准误估计.因此从集合的覆盖面积来看，普通版的置信集合是稳健版的子集。
```{r plot_conf_robset, fig.show='hide'}
confidenceEllipse(model, 
                  fill = T,
                  lwd = 0,
                  which.coef = c("STR", "expenditure"),
                  main = "95% Confidence Sets",
                  vcov. = vcovHC(model, type = "HC1"),
                  col = "red")
                  
confidenceEllipse(model, 
                  fill = T,
                  lwd = 0,
                  which.coef = c("STR", "expenditure"),
                  add = T)
```
]

.pull-right[
```{r ref.label = 'plot_conf_robset', echo=F, fig.height=7}
```
]

---
# 多元回归的模型设定
模型设定(model specification) 一般涉及两个问题：

1. 用哪些X变量
2. 用线性模型还是非线性模型。要不要加入交叉项或者高阶项。

这两个问题依然在OVB范畴中，我们需要权衡的就是偏差和方差的关系。班级规模的回归中我们加入一个参数
```{r}
model <- lm(score ~ STR + english + lunch, data = CASchools)
coeftest(model, vcov. = vcovHC, type = "HC1")
```

---
# 模型设定
估计模型
$$\widehat{TestScore} = \underset{(5.56)}{700.15} - \underset{(0.27)}{1.00} \times size - \underset{(0.03)}{0.12} \times english - \underset{(0.02)}{0.55} \times lunch.$$
注意到两点：
1. 新加入的参数lunch的系数估计是显著的。
2. 加入该参数之后，我们所关心的size系数的估计没有大幅度的变动，并且仍然保持显著性。

在这种情况下，在模型中保留该新参数lunch是适合的。

---
# 关于R方与调整R方

无论是R方还是调整R方，它们告诉我们的知识所选取的变量集合能够解释Y方差的比例。它并没有告诉我们
1.  某个X变量是否显著。

2. 某个X变量是否与Y变量有因果联系。

3. 不存在忽略变量偏误，X变量已经足够

4. 目前所选取的X变量集合是最合适的。

现实中可能存在很多伪回归 (spurious regression)的问题，也就是R方很高的回归，但其实X和Y之间毫无因果联系。

---
# 伪回归
.pull-left[
构造虚拟变量PLS标识城市中停车位的个数，与某学区的收入，消费，班级规模等相关，但与Y无关。
```{r plot_sup_reg, fig.show='hide'}
CASchools$PLS <- c(22 * CASchools$income 
                   - 15 * CASchools$STR 
                   + 0.2 * CASchools$expenditure
                   + rnorm(nrow(CASchools), sd = 80) + 3000)
# plot parking lot space against test score
plot(CASchools$PLS, 
     CASchools$score,
     xlab = "Parking Lot Space",
     ylab = "Test Score",
     pch = 20,
     col = "steelblue")    
```
]

.pull-right[
```{r ref.label = 'plot_sup_reg', echo=F, fig.height=7}
```
]

---
# 伪回归 (2)
增加的模拟变量PLS与Y没有任何联系，但在与Y的回归后，获得了显著的参数估计和较高的R方。
```{r}
summary(lm(score ~ PLS, data = CASchools))
```

---
# 思考：
利用以上生成的伪变量PLS，检查以下两段回归代码，你有何观察？
```{r eval=FALSE}
model_1 <- lm(score ~ STR + english + lunch+PLS, data = CASchools)
summary(model_1)
model_2 <- lm(score ~ STR + english + lunch + PLS + income + expenditure, data = CASchools)
summary(model_2)
```
**提示**：
1. 新加入的任何变量，要观察其本身的显著性，对其他参数估计显著性的影响，对R方的影响。
2. 如果新加入一个变量之后，导致原先的显著性格局发生了明显的变化，意味着这个变量有问题。
3. 多重共线性的问题。

---
# 相关系数与散点图
<iframe src="corr_scat.html" width="150%" height="90%" frameBorder="0"></iframe>

---
# 模型比较和展示输出
我们循序渐进的考虑以下五种模型，每个模型增加一个解释变量

$$\begin{align*}
  (I) \quad TestScore=& \, \beta_0 + \beta_1 \times size + u, \\
  (II) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + u, \\
  (III) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times lunch + u, \\
  (IV) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_4 \times calworks + u, \\
  (V) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times lunch + \beta_4 \times calworks + u
\end{align*}$$

主要结论：
1. 增加控制变量可以将STR的边际效应减半，但是不影响其显著性。
2. 增加控制变量可以显著提升模型的解释力，R方从0.05增加到0.77。
3. lunch和calworks不需要同时作为控制变量。前者更应该留在模型中。

---
```{r echo=F}
library(sjPlot)
library(sjmisc)
library(sjlabelled)
m1 <- lm(score ~ STR, data = CASchools)
m2 <- lm(score ~ STR + english, data = CASchools)
m3 <- lm(score ~ STR + english + lunch, data = CASchools)
m4 <- lm(score ~ STR + english + calworks, data = CASchools)
m5 <- lm(score ~ STR + english + lunch + calworks, data = CASchools)
tab_model(m1, m2, m3, m4, m5, robust = T, emph.p = T,
          show.fstat = T, title = "Regression Outputs",
          show.ci = F, collapse.se = T, 
          p.style = "a")
```