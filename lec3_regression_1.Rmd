---
title: "回归分析"
subtitle: "<单变量回归>"
author: "冯凌秉"
institute: "<span style = 'font-size: 70%;'>
  江西财经大学 <br> 
  产业经济研究院</span>"
date: '2020<br><br>
  `r icon::fa("paper-plane")` <feng.lingbing@jxufe.edu.cn>'
output:
  xaringan::moon_reader:
    css: [default, zh-CN.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = ">", 
                      fig.retina = 3, warning = FALSE, message = FALSE)
```

# 单变量回归

$$Y_i = \beta_0 + \beta_1 X_i + u_i$$
- i是观测值的标签 $i = 1, 2, \cdots, n$
- $Y$是被解释变量 (dependent variable), $X$是解释变量，或回归变量 (regressor)
- $Y = \beta_0 + \beta_1 X$ 是总体回归线/函数 (population regression line/function)
- $\beta_0$ 是回归线的截距项, $\beta_1$是回归线的斜率
- $\mu_i$ 是误差项

.pull-left[
STR是某个班级的学生教师比, TestScore是班级学生的平均成绩。总体回归函数为 $TestScore = 713 - 3 \times STR.$
```{r plot_1, fig.show = 'hide'}
STR <- c(15, 17, 19, 20, 22, 23.5, 25)
TestScore <- c(680, 640, 670, 660, 630, 660, 635)
plot(TestScore ~ STR)
abline(a = 713, b = -3)
```
]


.pull-right[
```{r ref.label = 'plot_1', echo=F, fig.height=5}
```
]


---
# 单变量回归 （2）
在上面的例子中，我们知道真实的总体回归函数的截距和斜率。但实际情况是我们永远不知道真实的回归函数是什么，而必须用获得的收据估计出函数的表达式。

我们看一下真实的数据：

.pull-left[
```{r plot_2, fig.show = 'hide'}
library(AER)
data(CASchools)
CASchools$STR <- CASchools$students/CASchools$teachers 
CASchools$score <- (CASchools$read + CASchools$math)/2
cor(CASchools$STR, CASchools$score)
plot(score ~ STR, 
     data = CASchools,
     main = "Scatterplot of TestScore and STR", 
     xlab = "STR (X)",
     ylab = "Test Score (Y)")
```
]



.pull-right[
两个变量的存在负相关关系，但是较弱

```{r ref.label = 'plot_2', echo=F, fig.height=5}
```
]

---
# 最小二乘估计
对于上图，我们希望找到一条直线，所有的观测点都尽可能靠近这条直线。如果有很多直线选择，也就是所有到这某条直线的距离之和是所有选择中最小的。

希望找到截距和斜率的估计值 $[\hat{\beta_0},\hat{\beta_1}]$, 可以最小化所谓的距离之和
$$
\sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2.
$$
这样的估计称为普通最小二乘估计 (OLS)，得到的估计值称为最小二乘估计值：

$$
\begin{align}
  \hat\beta_1 & = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2},  \\
  \\
  \hat\beta_0 & =  \overline{Y} - \hat\beta_1 \overline{X}. 
\end{align}
$$
---
# 单变量回归：lm() 函数

R中做线性回归的主力工具是`lm()`函数，需要至少提供两个信息给该函数：1是回归的公式称为formula，另一个是数据的来源（一般是一个 data frame）
.pull-left[
```{r plot_3, fig.show = 'hide'}
linear_model <- lm(score ~ STR, data = CASchools)
plot(score ~ STR, 
     data = CASchools,
     main = "Scatterplot of TestScore and STR", 
     xlab = "STR (X)",
     ylab = "Test Score (Y)",
     xlim = c(10, 30),
     ylim = c(600, 720))

# add the regression line
abline(linear_model) 
```
]

.pull-right[
```{r ref.label = 'plot_3', echo=F, fig.height=5}
```
]

一个自然的问题是：如何评估回归效果的好坏。什么是回归的效果？

从一堆数据中估计出一条直线的过程，我们一般称为拟合 (fit)。回归的效果可以先从拟合的优劣程度来说明。

---
# 拟合优度：R方
R方定义：Y的方差中能够被回归函数所解释的比例。用数学描述
$$\begin{align}
  ESS & =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  TSS & =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  TSS &= ESS + SSR \\
  R^2 & = \frac{ESS}{TSS} = 1- \frac{SSR}{TSS} \\
  SSR & = \sum_{i=1}^n \hat{u}_i^2.
\end{align}$$

.center[R方的取值位于0和1之间。]

SSR被称作残差平方和。残差平方和与R方是对应的。

---
# 回归标准误

回归标准误 (standard error of regression, SER) 是回归残差 $\hat{u}_i$ 的标准差的估计值
$$SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2}$$
误差项 $\mu_i$ 本身是**不可观测**的，因此我们使用残差 $\hat{u}_i$ 对其进行估计。

.pull-left[
```{r}
(mod_summary <- summary(linear_model))  
```
]

.pull-right[
左侧使用`summary`函数可以查看lm模型回归的结果
下面根据公式手动计算R方和回归标准误，并比较summary的结果。
```{r}
SSR <- sum(mod_summary$residuals^2)
TSS <- sum((CASchools$score - mean(CASchools$score))^2)
(R2 <- 1 - SSR/TSS)
n <- nrow(CASchools)
(SER <- sqrt(SSR / (n-2)))
```
]

---
class: center, inverse

# 最小二乘假设 

### 1. $\mu_i$ 的条件均值为0: $E(u_i|X_i) = 0.$

### 2. $(X_i,Y_i), i = 1,\dots,n$ 是从其联合分布中抽取的具有独立同分布 (i.i.d.) 的样本。

### 3. 没有显著的异常值，X和Y变量具有有限的四阶矩。

#### 如果这些条件没有满足，那么即便在大样本情况下，估计值也可能是非正态分布的。

.center[<font face="verdana", size="6" color="steelblue">不是正态分布有什么大不了？</font>]


---
# 条件均值为0假设

.pull-left[
```{r plot_4, fig.show = 'hide'}
set.seed(321)
X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 5)  
Y <- X^2 + 2 * X + u                
mod_simple <- lm(Y ~ X)
prediction <- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X)))
plot(Y ~ X)
abline(mod_simple, col = "red")
lines(sort(X), prediction)
# 无条件均值为0
mean(resid(mod_simple))
```

无条件均值为0，但是条件均值不为0。

比如给定 $X<0$, $E(\hat{\mu_i} \vert X<0) > 0$ 
]

.pull-right[
```{r ref.label = 'plot_4', echo=F, fig.height=6}
```
]

---
# 独立同分布假设

.pull-left[
模拟如下回归函数：
$$X_t = -5 + 0.98 \cdot X_{t-1} + u_t$$
```{r plot_5, fig.show = 'hide'}
set.seed(123)
Date <- seq(as.Date("1951/1/1"), as.Date("2000/1/1"), "years")
X <- c(5000, rep(NA, length(Date)-1))
for (i in 2:length(Date)) {
    X[i] <- -50 + 0.98 * X[i-1] + rnorm(n = 1, sd = 200)
}
plot(x = Date, y = X, type = "l", 
     col = "steelblue", ylab = "Workers", 
     xlab = "Time")
```

思考：此处，为什么违反了独立同分布假设？
]

.pull-right[
```{r ref.label = 'plot_5', echo=F, fig.height=6}
```
]


---
# 异常值对回归的影响

<iframe src="https://www.econometrics-with-r.org/Outlier.html" width="100%" height="70%" frameBorder="0"></iframe>

---
# OLS 估计值的抽样分布

如果以上前提假设满足，那么在大样本下
1. 截距和效率估计量都是无偏的：
$$
E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,
$$
2. $\beta_1$ 为正态分布 $\mathcal{N}(\beta_1, \sigma^2_{\hat\beta_1})$：
$$
\begin{align}
\sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2}. \tag{4.1}
\end{align}
$$
3. $\beta_0$ 为正态分布 $\mathcal{N}(\beta_0, \sigma^2_{\hat\beta_0})$：
$$
\begin{align}
\sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2}
\end{align}
$$
---
## 斜率估计量的抽样分布


<iframe src="https://www.econometrics-with-r.org/SmallSampleDIstReg.html" width="100%" height="80%" frameBorder="0"></iframe>


---
# Var (x) 的重要性
.pull-left[
由公式(4.1) 可知 $\beta_1$ 估计的有效性与X变量的方差成反比，X的方差越大（信息量越大），该估计的标准误越小。

模拟验证之：假设样本 $(x_i, y_i), 1=1, 2, \cdots, 100$ 来自于联合正态分布的两个变量X和Y，并且:
$$E(X)=E(Y)=5$$
$$Var(X)=Var(Y)=5$$
$$Cov(X,Y)=4$$

将该样本切分为两个部分，第一部分满足 $\lvert X - \overline{X} \rvert > 1$, 也就是X的中间观测部分，剩下的两边为第二部分。
]

.pull-right[
蓝色点为第一部分观测值，其位于X的均值附近，方差较小；黑色点是第二部分观测值，其方差明显更大。
```{r echo=F}
# load the MASS package
library(MASS)
set.seed(4)
bvndata <- mvrnorm(100, 
                mu = c(5, 5), 
                Sigma = cbind(c(5, 4), c(4, 5))) 
colnames(bvndata) <- c("X", "Y")
bvndata <- as.data.frame(bvndata)
set1 <- subset(bvndata, abs(mean(X) - X) > 1)
set2 <- subset(bvndata, abs(mean(X) - X) <= 1)
plot(set1, 
     xlab = "X", 
     ylab = "Y", 
     pch = 19)
points(set2, 
       col = "steelblue", 
       pch = 19)
```
]

---
# Var(X) 的重要性

.pull-left[

明显的，黑色回归线的效果要好于蓝色回归线。

蓝色线条并没有发现X和Y的真实回归关系的原因是因为信息量太少。

分别用两部分数据拟合线性回归模型

并添加回归线。



```{r eval=F}
lm.set1 <- lm(Y ~ X, data = set1)
lm.set2 <- lm(Y ~ X, data = set2)
coef(summary(lm.set1))
coef(summary(lm.set2))
plot(set1, xlab = "X", ylab = "Y", pch = 19)
points(set2, col = "steelblue", pch = 19)
abline(lm.set1, col = "black")
abline(lm.set2, col = "steelblue")  
```
]

.pull-right[

```{r echo = FALSE}
lm.set1 <- lm(Y ~ X, data = set1)
lm.set2 <- lm(Y ~ X, data = set2)
coef(summary(lm.set1))
coef(summary(lm.set2))
plot(set1, xlab = "X", ylab = "Y", pch = 19)
points(set2, col = "steelblue", pch = 19)
abline(lm.set1, col = "black")
abline(lm.set2, col = "steelblue") 
```
]

























