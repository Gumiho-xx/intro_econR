<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>回归分析</title>
    <meta charset="utf-8" />
    <meta name="author" content="冯凌秉" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="zh-CN.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 回归分析
## <单变量回归>
### 冯凌秉
### <span style="font-size: 70%;"> 江西财经大学 <br> 产业经济研究院</span>
### 2020<br><br> <i class="fas  fa-paper-plane "></i> <a href="mailto:feng.lingbing@jxufe.edu.cn" class="email">feng.lingbing@jxufe.edu.cn</a>

---



# 单变量回归

`$$Y_i = \beta_0 + \beta_1 X_i + u_i$$`
- i是观测值的标签 `\(i = 1, 2, \cdots, n\)`
- `\(Y\)`是被解释变量 (dependent variable), `\(X\)`是解释变量，或回归变量 (regressor)
- `\(Y = \beta_0 + \beta_1 X\)` 是总体回归线/函数 (population regression line/function)
- `\(\beta_0\)` 是回归线的截距项, `\(\beta_1\)`是回归线的斜率
- `\(\mu_i\)` 是误差项

.pull-left[
STR是某个班级的学生教师比, TestScore是班级学生的平均成绩。总体回归函数为 `\(TestScore = 713 - 3 \times STR.\)`

```r
STR &lt;- c(15, 17, 19, 20, 22, 23.5, 25)
TestScore &lt;- c(680, 640, 670, 660, 630, 660, 635)
plot(TestScore ~ STR)
abline(a = 713, b = -3)
```
]


.pull-right[
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-1-1.png" width="504" /&gt;
]


---
# 单变量回归 （2）
在上面的例子中，我们知道真实的总体回归函数的截距和斜率。但实际情况是我们永远不知道真实的回归函数是什么，而必须用获得的收据估计出函数的表达式。

我们看一下真实的数据：

.pull-left[

```r
library(AER)
data(CASchools)
CASchools$STR &lt;- CASchools$students/CASchools$teachers 
CASchools$score &lt;- (CASchools$read + CASchools$math)/2
cor(CASchools$STR, CASchools$score)
&gt; [1] -0.2263627
plot(score ~ STR, 
     data = CASchools,
     main = "Scatterplot of TestScore and STR", 
     xlab = "STR (X)",
     ylab = "Test Score (Y)")
```
]



.pull-right[
两个变量的存在负相关关系，但是较弱


```
&gt; [1] -0.2263627
```

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-2-1.png" width="504" /&gt;
]

---
# 最小二乘估计
对于上图，我们希望找到一条直线，所有的观测点都尽可能靠近这条直线。如果有很多直线选择，也就是所有到这某条直线的距离之和是所有选择中最小的。

希望找到截距和斜率的估计值 `\([\hat{\beta_0},\hat{\beta_1}]\)`, 可以最小化所谓的距离之和
$$
\sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2.
$$
这样的估计称为普通最小二乘估计 (OLS)，得到的估计值称为最小二乘估计值：

$$
`\begin{align}
  \hat\beta_1 &amp; = \frac{ \sum_{i = 1}^n (X_i - \overline{X})(Y_i - \overline{Y}) } { \sum_{i=1}^n (X_i - \overline{X})^2},  \\
  \\
  \hat\beta_0 &amp; =  \overline{Y} - \hat\beta_1 \overline{X}. 
\end{align}`
$$
---
# 单变量回归：lm() 函数

R中做线性回归的主力工具是`lm()`函数，需要至少提供两个信息给该函数：1是回归的公式称为formula，另一个是数据的来源（一般是一个 data frame）
.pull-left[

```r
linear_model &lt;- lm(score ~ STR, data = CASchools)
plot(score ~ STR, 
     data = CASchools,
     main = "Scatterplot of TestScore and STR", 
     xlab = "STR (X)",
     ylab = "Test Score (Y)",
     xlim = c(10, 30),
     ylim = c(600, 720))

# add the regression line
abline(linear_model) 
```
]

.pull-right[
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;
]

一个自然的问题是：如何评估回归效果的好坏。什么是回归的效果？

从一堆数据中估计出一条直线的过程，我们一般称为拟合 (fit)。回归的效果可以先从拟合的优劣程度来说明。

---
# 拟合优度：R方
R方定义：Y的方差中能够被回归函数所解释的比例。用数学描述
`$$\begin{align}
  ESS &amp; =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  TSS &amp; =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  TSS &amp;= ESS + SSR \\
  R^2 &amp; = \frac{ESS}{TSS} = 1- \frac{SSR}{TSS} \\
  SSR &amp; = \sum_{i=1}^n \hat{u}_i^2.
\end{align}$$`

.center[R方的取值位于0和1之间。]

SSR被称作残差平方和。残差平方和与R方是对应的。

---
# 回归标准误

回归标准误 (standard error of regression, SER) 是回归残差 `\(\hat{u}_i\)` 的标准差的估计值
`$$SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{where} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2}$$`
误差项 `\(\mu_i\)` 本身是**不可观测**的，因此我们使用残差 `\(\hat{u}_i\)` 对其进行估计。

.pull-left[

```r
(mod_summary &lt;- summary(linear_model))  
&gt; 
&gt; Call:
&gt; lm(formula = score ~ STR, data = CASchools)
&gt; 
&gt; Residuals:
&gt;     Min      1Q  Median      3Q     Max 
&gt; -47.727 -14.251   0.483  12.822  48.540 
&gt; 
&gt; Coefficients:
&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
&gt; (Intercept) 698.9329     9.4675  73.825  &lt; 2e-16 ***
&gt; STR          -2.2798     0.4798  -4.751 2.78e-06 ***
&gt; ---
&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
&gt; 
&gt; Residual standard error: 18.58 on 418 degrees of freedom
&gt; Multiple R-squared:  0.05124,	Adjusted R-squared:  0.04897 
&gt; F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06
```
]

.pull-right[
左侧使用`summary`函数可以查看lm模型回归的结果
下面根据公式手动计算R方和回归标准误，并比较summary的结果。

```r
SSR &lt;- sum(mod_summary$residuals^2)
TSS &lt;- sum((CASchools$score - mean(CASchools$score))^2)
(R2 &lt;- 1 - SSR/TSS)
&gt; [1] 0.05124009
n &lt;- nrow(CASchools)
(SER &lt;- sqrt(SSR / (n-2)))
&gt; [1] 18.58097
```
]

---
class: center, inverse

# 最小二乘假设 

### 1. `\(\mu_i\)` 的条件均值为0: `\(E(u_i|X_i) = 0.\)`

### 2. `\((X_i,Y_i), i = 1,\dots,n\)` 是从其联合分布中抽取的具有独立同分布 (i.i.d.) 的样本。

### 3. 没有显著的异常值，X和Y变量具有有限的四阶矩。

#### 如果这些条件没有满足，那么即便在大样本情况下，估计值也可能是非正态分布的。

.center[&lt;font face="verdana", size="6" color="steelblue"&gt;不是正态分布有什么大不了？&lt;/font&gt;]


---
# 条件均值为0假设

.pull-left[

```r
set.seed(321)
X &lt;- runif(50, min = -5, max = 5)
u &lt;- rnorm(50, sd = 5)  
Y &lt;- X^2 + 2 * X + u                
mod_simple &lt;- lm(Y ~ X)
prediction &lt;- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X)))
plot(Y ~ X)
abline(mod_simple, col = "red")
lines(sort(X), prediction)
# 无条件均值为0
mean(resid(mod_simple))
&gt; [1] 1.023764e-15
```

无条件均值为0，但是条件均值不为0。

比如给定 `\(X&lt;0\)`, `\(E(\hat{\mu_i} \vert X&lt;0) &gt; 0\)` 
]

.pull-right[
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-6-1.png" width="504" /&gt;

```
&gt; [1] 1.023764e-15
```
]

---
# 独立同分布假设

.pull-left[
模拟如下回归函数：
`$$X_t = -5 + 0.98 \cdot X_{t-1} + u_t$$`

```r
set.seed(123)
Date &lt;- seq(as.Date("1951/1/1"), as.Date("2000/1/1"), "years")
X &lt;- c(5000, rep(NA, length(Date)-1))
for (i in 2:length(Date)) {
    X[i] &lt;- -50 + 0.98 * X[i-1] + rnorm(n = 1, sd = 200)
}
plot(x = Date, y = X, type = "l", 
     col = "steelblue", ylab = "Workers", 
     xlab = "Time")
```

思考：此处，为什么违反了独立同分布假设？
]

.pull-right[
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-7-1.png" width="504" /&gt;
]


---
# 异常值对回归的影响

&lt;iframe src="https://www.econometrics-with-r.org/Outlier.html" width="100%" height="70%" frameBorder="0"&gt;&lt;/iframe&gt;

---
# OLS 估计值的抽样分布

如果以上前提假设满足，那么在大样本下
1. 截距和效率估计量都是无偏的：
$$
E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,
$$
2. `\(\beta_1\)` 为正态分布 `\(\mathcal{N}(\beta_1, \sigma^2_{\hat\beta_1})\)`：
$$
`\begin{align}
\sigma^2_{\hat\beta_1} = \frac{1}{n} \frac{Var \left[ \left(X_i - \mu_X \right) u_i  \right]}  {\left[  Var \left(X_i \right)  \right]^2}. \tag{4.1}
\end{align}`
$$
3. `\(\beta_0\)` 为正态分布 `\(\mathcal{N}(\beta_0, \sigma^2_{\hat\beta_0})\)`：
$$
`\begin{align}
\sigma^2_{\hat\beta_0} =  \frac{1}{n} \frac{Var \left( H_i u_i \right)}{ \left[  E \left(H_i^2  \right)  \right]^2 } \ , \ \text{where} \ \ H_i = 1 - \left[ \frac{\mu_X} {E \left( X_i^2\right)} \right] X_i. \tag{4.2}
\end{align}`
$$
---
## 斜率估计量的抽样分布


&lt;iframe src="https://www.econometrics-with-r.org/SmallSampleDIstReg.html" width="100%" height="80%" frameBorder="0"&gt;&lt;/iframe&gt;


---
# Var (x) 的重要性
.pull-left[
由公式(4.1) 可知 `\(\beta_1\)` 估计的有效性与X变量的方差成反比，X的方差越大（信息量越大），该估计的标准误越小。

模拟验证之：假设样本 `\((x_i, y_i), 1=1, 2, \cdots, 100\)` 来自于联合正态分布的两个变量X和Y，并且:
`$$E(X)=E(Y)=5$$`
`$$Var(X)=Var(Y)=5$$`
`$$Cov(X,Y)=4$$`

将该样本切分为两个部分，第一部分满足 `\(\lvert X - \overline{X} \rvert &gt; 1\)`, 也就是X的中间观测部分，剩下的两边为第二部分。
]

.pull-right[
蓝色点为第一部分观测值，其位于X的均值附近，方差较小；黑色点是第二部分观测值，其方差明显更大。
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-8-1.png" width="504" /&gt;
]

---
# Var(X) 的重要性

.pull-left[

明显的，黑色回归线的效果要好于蓝色回归线。

蓝色线条并没有发现X和Y的真实回归关系的原因是因为信息量太少。

分别用两部分数据拟合线性回归模型

并添加回归线。




```r
lm.set1 &lt;- lm(Y ~ X, data = set1)
lm.set2 &lt;- lm(Y ~ X, data = set2)
coef(summary(lm.set1))
coef(summary(lm.set2))
plot(set1, xlab = "X", ylab = "Y", pch = 19)
points(set2, col = "steelblue", pch = 19)
abline(lm.set1, col = "black")
abline(lm.set2, col = "steelblue")  
```
]

.pull-right[


```
&gt;              Estimate Std. Error  t value     Pr(&gt;|t|)
&gt; (Intercept) 1.4089745 0.35605337  3.95720 1.836574e-04
&gt; X           0.7120473 0.06058566 11.75274 4.928842e-18
&gt;                Estimate Std. Error    t value   Pr(&gt;|t|)
&gt; (Intercept)  5.31290574  2.2918140  2.3182098 0.02796202
&gt; X           -0.04584703  0.4417761 -0.1037789 0.91808461
```

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-10-1.png" width="504" /&gt;
]

---
# 斜率系数的假设检验
原假设 `\(H_0: \beta_1 = \beta_{1,0}\)`。备择假设为 `\(H_1:\beta_1 \neq \beta_{1,0}\)`。步骤如下：
1. 计算 `\(\hat{\beta}_1\)` 的标准误：
`$$SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}.$$`
2. 计算t统计量值
`$$t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }.$$`
3.如果t统计量大于1.96，或者p值（计算公式如下）小于0.05，拒绝原假设。
`$$\begin{align*}
    p \text{-value} =&amp; \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| &gt; \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] 
    \approx 2 \cdot \Phi(-|t^{act}|)
  \end{align*}$$`
  
---
# 实例

```r
data(CASchools)
CASchools$STR &lt;- CASchools$students/CASchools$teachers
CASchools$score &lt;- (CASchools$read + CASchools$math)/2
linear_model &lt;- lm(score ~ STR, data = CASchools)
summary(linear_model)$coefficients
&gt;               Estimate Std. Error   t value      Pr(&gt;|t|)
&gt; (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
&gt; STR          -2.279808  0.4798255 -4.751327  2.783308e-06
```

p值接近于0，意味着如果原假设 `\(H_0: \beta_1 = 0\)` 成立，假设我们可以获得多次观测值并且估计模型的系数，那么我们能得到 `\(\hat\beta_1 \geq |-2.28|\)` 的概率接近于0。

然而实际的观测数据计算出来的 `\(\hat\beta_1 = -2.28\)`, 也就意味着在原假设成立的情况下，发生了几乎不可能发生的事情 (即小概率时间)。

因此我们应该拒绝原假设。

---
# p值演示
书中的代码较繁琐，建议大家一步一步运行，搞清楚每一步的作用。
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-12-1.png" width="792" style="display: block; margin: auto;" /&gt;

---
# 斜率参数估计的置信区间
斜率参数 `\(\beta_i\)` 的95%置信水平的置信区间为

`$$\text{CI}_{0.95}^{\beta_i} = \left[ \hat{\beta}_i - 1.96 \times SE(\hat{\beta}_i) \, , \, \hat{\beta}_i + 1.96 \times SE(\hat{\beta}_i) \right]$$`

在R中，用`lm`建立线性模型之后，可以通过`confint`函数轻松获得置信区间：

```r
confint(linear_model)
&gt;                 2.5 %     97.5 %
&gt; (Intercept) 680.32312 717.542775
&gt; STR          -3.22298  -1.336636
```

看其与理论值是否相等

```r
lm_summ &lt;- summary(linear_model)
c("lower" = lm_summ$coef[2,1] - qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2],
  "upper" = lm_summ$coef[2,1] + qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2])
&gt;     lower     upper 
&gt; -3.222980 -1.336636
```

---
# 置信区间的频率和概率解释
在频率学派看来，用上述公式计算出的置信区间只能用频率解释。因为每一个区间要么包含了真值，要么不包含真值。下图红色的置信区间即为不包含真值5的置信区间。可以看出如果抽样100次，有5个左右的置信区间如此。
.pull-left[

```r
lower &lt;- numeric(10000); upper &lt;- numeric(10000)
for(i in 1:10000) {
  Y &lt;- rnorm(100, mean = 5, sd = 5)
  lower[i] &lt;- mean(Y) - 1.96 * 5 / 10
  upper[i] &lt;- mean(Y) + 1.96 * 5 / 10
}
CIs &lt;- cbind(lower, upper)
ID &lt;- which(!(CIs[1:100, 1] &lt;= 5 &amp; 5 &lt;= CIs[1:100, 2]))
plot(0, xlim = c(3, 7), ylim = c(1, 100), 
     ylab = "Sample", xlab = expression(mu), 
     main = "Confidence Intervals")
colors &lt;- rep(gray(0.6), 100)
colors[ID] &lt;- "red"
abline(v = 5, lty = 2)
for(j in 1:100) {
  lines(c(CIs[j, 1], CIs[j, 2]), 
        c(j, j), col = colors[j], 
        lwd = 2)
}
```
]

.pull-right[
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-15-1.png" width="504" /&gt;
]

---
# X 为二值变量
如下回归中 `\(D_i\)` 是一个二值变量 (binary variable)，也称为哑变量 (dummy variable)，它只可以有两种取值。
$$
Y_i = \beta_0 + \beta_1 D_i + u_i 
$$
如果STR &lt; 20, `\(D_i = 1\)`，否则 `\(D_i = 0\)`。那么回归模型变为:
`$$TestScore_i = \beta_0 + \beta_1 D_i + u_i$$`
.pull-left[

```r
CASchools$D &lt;- CASchools$STR &lt; 20
plot(CASchools$D, CASchools$score, 
     pch = 20, cex = 0.5,                               
     col = "Steelblue",                       
     xlab = expression(D[i]),                
     ylab = "Test Score",
     main = "Dummy Regression")
```
]

.pull-right[
&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-16-1.png" width="504" /&gt;
]

---
# 参数的解释
在X变量是二值变量时，不能再将其参数系数解释为斜率。因为在X的取值空间上，只有两个点0和1是非空的。
- `\(E(Y_i | D_i = 0) = \beta_0\)`: `\(\beta_0\)` 解释为当STR小于20时的测试分数的期望值。
- `\(E(Y_i | D_i = 1) = \beta_0 + \beta_1\)` 为两个不同小组测试分数期望值的差距。
- 也就是STR小于20的学生测试分数的期望值比STR大于等于20的学生测试分数的期望值大 `\(\beta_1\)` (如果 `\(\beta_1 &gt; 0\)`)

```r
dummy_model &lt;- lm(score ~ D, data = CASchools)
coef(summary(dummy_model))
&gt;               Estimate Std. Error    t value     Pr(&gt;|t|)
&gt; (Intercept) 650.076798   1.393023 466.666105 0.0000000000
&gt; DTRUE         7.169435   1.846648   3.882405 0.0001201701
```
$$
\hat{\beta_0} = 650.1, \hat{\beta_1} = 7.17
$$
&lt;font color = #990000&gt; [问题]：请给出两个参数估计值的现实解释，并提取两个参数95%置信区间。&lt;/font&gt;

---
# 同方差假设
回归模型另外一个重要的假设是误差线的条件方差是定值 (homoskedastic)
$$
\text{Var}(u_i|X_i=x) = \sigma^2 \ \forall \ i=1,\dots,n.
$$
如果该条件方差与X的观测顺序有关，则称为异方差 (heteroskedastic)
$$
\text{Var}(u_i|X_i=x) = \sigma_i^2 \ \forall \ i=1,\dots,n.
$$
---
# 模拟异方差现象
.pull-left[

```r
library(scales)
x &lt;- rep(c(10, 15, 20, 25), each = 25)
e &lt;- c()
e[1:25] &lt;- rnorm(25, sd = 10)
e[26:50] &lt;- rnorm(25, sd = 15)
e[51:75] &lt;- rnorm(25, sd = 20)
e[76:100] &lt;- rnorm(25, sd = 25)
y &lt;- 720 - 3.3 * x + e
mod &lt;- lm(y ~ x)
plot(x = x, y = y, 
     main = "An Example of Heteroskedasticity",
     xlab = "Student-Teacher Ratio",
     ylab = "Test Score",
     cex = 0.5, pch = 19, 
     xlim = c(8, 27), 
     ylim = c(600, 710))
abline(mod, col = "darkred")
boxplot(formula = y ~ x, 
        add = TRUE, at = c(10, 15, 20, 25), 
        col = alpha("gray", 0.4), 
        border = "black")
```
]

.pull-right[
随着观测值X的值变大，Y的方差也在变大，同样可以导致误差项的方差也随着X的增加而增加。

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-18-1.png" width="504" /&gt;
]
---
# 真实异方差实例
.pull-left[

```r
library(AER)
data("CPSSWEducation")
attach(CPSSWEducation)
labor_model &lt;- lm(earnings ~ education)
plot(education, 
     earnings, 
     ylim = c(0, 150))
abline(labor_model, 
       col = "steelblue", 
       lwd = 2)
detach(CPSSWEducation)
confint(labor_model)
&gt;                 2.5 %    97.5 %
&gt; (Intercept) -5.015248 -1.253495
&gt; education    1.330098  1.603753
```
]

.pull-right[

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-19-1.png" width="504" /&gt;

```
&gt;                 2.5 %    97.5 %
&gt; (Intercept) -5.015248 -1.253495
&gt; education    1.330098  1.603753
```
]

---
# 异方差的后果
在同方差假设下 (`summary`函数给出的标准误估计):
`$$\sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum_{i=1}^n(X_i - \overline{X})^2} } \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2$$`
但是在异方差情形，稳健(robust)的标准误估计是 (`sandwich`包中的`vcovHC()`函数提供了很多版本的标准误稳健估计，比如HC0估计 &lt;a name=cite-white1980&gt;&lt;/a&gt;([White, 1980](#bib-white1980)))

`$$SE(\hat{\beta}_1) = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2}}$$`

或者HC1估计&lt;a name=cite-mackinnon1985&gt;&lt;/a&gt;([MacKinnon and White, 1985](#bib-mackinnon1985))为
`$$\begin{align}
SE(\hat{\beta}_1)_{HC1} = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2}}
\end{align}$$`

---
利用`vcovHC`获得参数的稳健标准误，利用`lmtest`包里的`coeftest`函数，则可以基于稳健标准误对参数进行稳健的假设检验

```r
vcov &lt;- vcovHC(linear_model, type = "HC1")
(robust_se &lt;- sqrt(diag(vcov)))
&gt; (Intercept)         STR 
&gt;  10.3643617   0.5194893
coeftest(linear_model, vcov. = vcov)
&gt; 
&gt; t test of coefficients:
&gt; 
&gt;              Estimate Std. Error t value  Pr(&gt;|t|)    
&gt; (Intercept) 698.93295   10.36436 67.4362 &lt; 2.2e-16 ***
&gt; STR          -2.27981    0.51949 -4.3886 1.447e-05 ***
&gt; ---
&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
与非稳健的结果比较，有何区别？

```r
coeftest(linear_model)
&gt; 
&gt; t test of coefficients:
&gt; 
&gt;              Estimate Std. Error t value  Pr(&gt;|t|)    
&gt; (Intercept) 698.93295    9.46749 73.8245 &lt; 2.2e-16 ***
&gt; STR          -2.27981    0.47983 -4.7513 2.783e-06 ***
&gt; ---
&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---
# 异方差模拟实例

.pull-left[
`$$Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} \mathcal{N}(0,0.36 \cdot X_i^2)$$`
$$
\beta_1 = 1
$$


```r
X &lt;- 1:500
Y &lt;- rnorm(n = 500, mean = X, sd = 0.6 * X)
reg &lt;- lm(Y ~ X)
plot(x = X, y = Y, 
     pch = 19, 
     col = "steelblue", 
     cex = 0.8)
abline(reg, 
       col = "darkred", 
       lwd = 1.5)
```
]

.pull-right[

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-22-1.png" width="504" /&gt;
]

---
# 异方差的后果
非稳健参数假设检验

```r
library(car)
linearHypothesis(reg, hypothesis.matrix = "X = 1")$`Pr(&gt;F)`[2]
&gt; [1] 0.7318397
```

稳健的参数估计假设检验

```r
linearHypothesis(reg, hypothesis.matrix = "X = 1", white.adjust = "hc1")$`Pr(&gt;F)`[2]
&gt; [1] 0.7527921
```

使用非稳健的标准误的假设检验更容易犯第一类错误：拒绝一个真实的原假设。


---
# 稳健标准误与I类错误

使用稳健标准误的假设检验犯第一类错误的概率接近5%，

而不使用稳健标准误的假设检验犯I类错误的概率增加到了7.4%。


```r
t &lt;- c()
t.rob &lt;- c()
for (i in 1:10000) {
  X &lt;- 1:1000
  Y &lt;- rnorm(n = 1000, mean = X, sd = 0.6 * X)
  reg &lt;- lm(Y ~ X)
  t[i] &lt;- linearHypothesis(reg, "X = 1")$'Pr(&gt;F)'[2] &lt; 0.05
  t.rob[i] &lt;- linearHypothesis(reg, "X = 1", white.adjust = "hc1")$'Pr(&gt;F)'[2] &lt; 0.05
}
round(cbind(t = mean(t), t.rob = mean(t.rob)), 3)
&gt;          t t.rob
&gt; [1,] 0.074 0.053
```

---
# 高斯-马尔科夫定理

&lt;font color = "red"&gt; [定理]：在满足一些条件下，OLS估计量是最佳线性无偏估计量 (BLUE)。&lt;/font&gt;

线性估计量的定义如下：
`$$\overset{\sim}{\beta}_1 = \sum_{i=1}^n a_i Y_i$$`

权重 `\(a_i\)` 可以与 `\(X_i\)` 有关，但与 `\(Y_i\)` 无关。如果

`$$E(\overset{\sim}{\beta}_1 | X_1, \dots, X_n) = \beta_1$$`

则 `\(\overset{\sim}{\beta}_1\)` 是一个无偏的线性估计量。

- 高斯马尔科夫的结论是，在所有这些线性无偏估计量中，OLS估计量的标准误是最小的，也就是最有效的。

- 在这里权重的选取起着决定性的作用，OLS的权重决定了它的最优性。

- 因此高斯马尔科夫定理为OLS的优越性提供了证据。

- 但我们也应该充分认识到这个定理成立的条件，在不能得到满足的情况下，OLS的优越性也无从谈起。

---
# 模拟实例：BLUE
.pull-left[

```r
n &lt;- 100      
reps &lt;- 1e5
epsilon &lt;- 0.8
w &lt;- c(rep((1 + epsilon) / n, n / 2), 
       rep((1 - epsilon) / n, n / 2))
ols &lt;- rep(NA, reps)
weightedestimator &lt;- rep(NA, reps)
for (i in 1:reps) {
  y &lt;- rnorm(n)
  ols[i] &lt;- mean(y)
  weightedestimator[i] &lt;- crossprod(w, y)
}
plot(density(ols), 
     col = "purple", lwd = 3, 
     main = "Density of OLS and Weighted Estimator",
     xlab = "Estimates")
lines(density(weightedestimator), 
      col = "steelblue", lwd = 3) 
legend('topright', c("OLS", "Weighted"), 
       col = c("purple", "steelblue"), lwd = 3)
```
]

.pull-right[
略微改变一下权重 `\(w_i = \frac{1 \pm 0.8}{100}\)` 不改变估计量线性无偏的性质，但是它的有效应明显低于OLS估计了。

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-26-1.png" width="504" /&gt;
]

---
# 小样本与t统计量

在OLS假设满足，误差项服从正态分布并且没有异方差的条件下，截距和斜率参数的估计量是正态分布的，相关假设检验的统计量在小样本下服从t分布。下图是检验统计量的抽样分布与理论分布对比。（不是估计量的分布！）

&lt;img src="lec3_regression_1_files/figure-html/unnamed-chunk-27-1.png" width="720" style="display: block; margin: auto;" /&gt;







---
# 参考文献

&lt;a name=bib-mackinnon1985&gt;&lt;/a&gt;[MacKinnon, J. G. and H.
White](#cite-mackinnon1985) (1985). "Some heteroskedasticity-consistent
covariance matrix estimators with improved finite sample properties".
In: _Journal of econometrics_ 29.3, pp. 305-325.

&lt;a name=bib-white1980&gt;&lt;/a&gt;[White, H.](#cite-white1980) (1980). "A
heteroskedasticity-consistent covariance matrix estimator and a direct
test for heteroskedasticity". In: _Econometrica: journal of the
Econometric Society_, pp. 817-838.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  /* Replace <script> tags in slides area to make them executable
   *
   * Runs after post-processing of markdown source into slides and replaces only
   * <script>s on the last slide of continued slides using the .has-continuation
   * class added by xaringan. Finally, any <script>s in the slides area that
   * aren't executed are commented out.
   */
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container:not(.has-continuation) script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
  var scriptsNotExecuted = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container.has-continuation script'
  );
  if (!scriptsNotExecuted.length) return;
  for (var i = 0; i < scriptsNotExecuted.length; i++) {
    var comment = document.createComment(scriptsNotExecuted[i].outerHTML)
    scriptsNotExecuted[i].parentElement.replaceChild(comment, scriptsNotExecuted[i])
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(jxufe_logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 50px;
  height: 60px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
